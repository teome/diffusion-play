{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Experiments for the basics of nn and torch implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin1')\n",
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "n,m = x_train.shape\n",
    "n, n_feats = x_train.shape\n",
    "c = (y_train.max() + 1).item()\n",
    "nh = n_hidden = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers: x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, nh, c)\n",
    "preds = model(x_train)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Experimenting with the basics for logsoftmax -- yet again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup some vars\n",
    "n_train = 64\n",
    "bs = 4\n",
    "n_classes = 10\n",
    "probs = torch.randn((bs, n_classes))\n",
    "y = torch.randint(0, 9, (bs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.63, -4.00, -2.91, -2.85, -3.18, -1.69, -1.21, -1.24, -5.36, -3.80],\n",
       "        [-2.45, -1.72, -3.00, -2.75, -1.03, -3.16, -4.40, -2.63, -2.25, -3.46],\n",
       "        [-2.01, -4.50, -2.86, -3.37, -2.39, -1.48, -2.49, -1.64, -2.12, -3.04],\n",
       "        [-2.34, -4.65, -2.51, -2.66, -2.41, -3.98, -3.22, -2.17, -1.06, -2.02]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = probs\n",
    "a = x.max(1, keepdim=True).values\n",
    "x_norm = x - a\n",
    "numerator = x_norm\n",
    "logsumexp = x_norm.exp().sum(1, keepdim=True).log()\n",
    "numerator.shape, logsumexp.shape\n",
    "logsoftmax = numerator - logsumexp\n",
    "logsoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.63, -4.00, -2.91, -2.85, -3.18, -1.69, -1.21, -1.24, -5.36, -3.80],\n",
       "        [-2.45, -1.72, -3.00, -2.75, -1.03, -3.16, -4.40, -2.63, -2.25, -3.46],\n",
       "        [-2.01, -4.50, -2.86, -3.37, -2.39, -1.48, -2.49, -1.64, -2.12, -3.04],\n",
       "        [-2.34, -4.65, -2.51, -2.66, -2.41, -3.98, -3.22, -2.17, -1.06, -2.02]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check against a version that doesn't use the normalisation with max\n",
    "(torch.exp(x) / torch.exp(x).sum(1, keepdim=True)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.63, -4.00, -2.91, -2.85, -3.18, -1.69, -1.21, -1.24, -5.36, -3.80],\n",
       "        [-2.45, -1.72, -3.00, -2.75, -1.03, -3.16, -4.40, -2.63, -2.25, -3.46],\n",
       "        [-2.01, -4.50, -2.86, -3.37, -2.39, -1.48, -2.49, -1.64, -2.12, -3.04],\n",
       "        [-2.34, -4.65, -2.51, -2.66, -2.41, -3.98, -3.22, -2.17, -1.06, -2.02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap into function\n",
    "def log_softmax(x):\n",
    "    x_norm = x - x.max(1, keepdim=True).values\n",
    "    numerator = x_norm\n",
    "    logsumexp = x_norm.exp().sum(1, keepdim=True).log()\n",
    "    logsoftmax = numerator - logsumexp\n",
    "    return logsoftmax\n",
    "\n",
    "assert torch.allclose(log_softmax(probs), F.log_softmax(probs, 1)), 'did not find equality with torch in log_softmax implementation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.22) tensor(3.22) tensor(3.22)\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = log_softmax(probs)\n",
    "preds = log_likelihood[range(log_likelihood.shape[0]), y]\n",
    "nll = -preds.mean()\n",
    "\n",
    "print(nll, F.nll_loss(F.log_softmax(probs, 1), y), F.cross_entropy(probs, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap into function\n",
    "def cross_entropy(probs, targets):\n",
    "    log_likelihood = log_softmax(probs)\n",
    "    preds = log_likelihood[range(log_likelihood.shape[0]), targets]\n",
    "    nll = -preds.mean()\n",
    "    return nll\n",
    "\n",
    "assert torch.allclose(cross_entropy(probs, y), F.cross_entropy(probs, y)), 'did not find equality with torch in cross_entropy implementation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Basic training loop\n",
    " Can now just use the torch implementation of log_softmax and cross_entropy\n",
    " - Get model preds\n",
    " - compare against labels and calculate loss\n",
    " - calculate gradient of loss with respece to model params\n",
    " - update params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>),\n",
       " torch.Size([50, 10]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 50\n",
    "xb = x_train[:bs]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[:bs]\n",
    "yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025850929940455"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# About the same as this for random init model and c classes\n",
    "-np.log(1/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3, 9, 3, 5, 3, 8,\n",
       "        3, 5, 9, 5, 9, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels): return (preds.argmax(dim=1) == labels).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.08)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(loss, preds, yb): print(f'loss: {loss:.2f}, accuracy: {accuracy(preds, yb):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.30, accuracy: 0.08\n"
     ]
    }
   ],
   "source": [
    "xb, yb = x_train[:bs], y_train[:bs]\n",
    "preds = model(xb)\n",
    "report(loss_func(preds, yb), preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.11, accuracy: 0.96\n",
      "loss: 0.13, accuracy: 0.96\n",
      "loss: 0.10, accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for layer in model.layers:\n",
    "                if hasattr(layer, 'weight'):\n",
    "                    layer.weight -= lr * layer.weight.grad\n",
    "                    layer.bias -= lr * layer.bias.grad\n",
    "                    layer.weight.grad.zero_()  # TODO test if can just set to None for simplicity \n",
    "                    layer.bias.grad.zero_()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters in nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(\n",
      "  (foo): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[ 0.06,  0.35,  0.40],\n",
      "        [ 0.49, -0.39,  0.20],\n",
      "        [ 0.26, -0.53,  0.22],\n",
      "        [-0.37, -0.38, -0.05]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.11, -0.45, -0.21, -0.57], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "m1 = nn.Module()\n",
    "m1.foo = nn.Linear(3, 4)\n",
    "print(m1)\n",
    "print(list(m1.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foo', Linear(in_features=3, out_features=4, bias=True))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m1.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in, n_hidden)\n",
    "        self.l2 = nn.Linear(n_hidden, n_out)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x): return self.l2(self.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(n_feats, nh, c)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n",
      "relu: ReLU()\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_children(): print(f'{name}: {layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters(): print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(3):\n",
    "        for i in range(0, n, bs):\n",
    "            s = slice(i, min(n, i+bs))\n",
    "            xb, yb = x_train[s], y_train[s]\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= lr * p.grad\n",
    "                model.zero_grad()\n",
    "        report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.12, accuracy: 0.98\n",
      "loss: 0.07, accuracy: 0.98\n",
      "loss: 0.05, accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule:\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "\n",
    "    def __setattr__(self, k, v):\n",
    "        if not k.startswith('_'): self._modules[k] = v\n",
    "        super().__setattr__(k, v)\n",
    "\n",
    "    def __repr__(self): return f'{self._modules}'\n",
    "\n",
    "    def parameters(self):\n",
    "        for layer in self._modules.values(): yield from layer.parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = MyModule(m, nh, c)\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in mdl.parameters(): print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering modules with our Model class\n",
    "\n",
    "Can still use the list of layers approach, but have to register the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i, layer in enumerate(self.layers): super().add_module(f'layer{i}', layer)\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     for layer in self.layers: x = layer(x)\n",
    "    #     return x\n",
    "    \n",
    "    def forward(self, x): return reduce(lambda val, layer: layer(val), self.layers, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer1): ReLU()\n",
       "  (layer2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList does this and nn.Sequential wraps it all in a single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers: x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialModel(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.13, accuracy: 0.94\n",
      "loss: 0.08, accuracy: 0.98\n",
      "loss: 0.04, accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.15, accuracy: 0.94\n",
      "loss: 0.06, accuracy: 1.00\n",
      "loss: 0.05, accuracy: 0.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.01, grad_fn=<NllLossBackward0>), tensor(1.))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5): self.params, self.lr = list(params), lr\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= self.lr * p.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.14, accuracy: 0.94\n",
      "loss: 0.13, accuracy: 0.96\n",
      "loss: 0.09, accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just use the functionality from torch now in torch.optim.XXX e.g. optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.14, accuracy: 0.94\n",
      "loss: 0.10, accuracy: 0.94\n",
      "loss: 0.10, accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = x_valid[s], y_valid[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Wrap both the inputs and targets into a single object for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "assert(len(train_ds) == len(x_train))\n",
    "xb, yb = train_ds[:5]\n",
    "assert(xb.shape == (5, *x_train[0].shape))\n",
    "assert(yb.shape == (5, *y_train[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.12, accuracy: 0.94\n",
      "loss: 0.09, accuracy: 0.98\n",
      "loss: 0.07, accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        xb, yb = train_ds[i:i+bs]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader class to handle datasets, batching, sampling etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, ds, bs): self.ds, self.bs = ds, bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 784]), torch.Size([50]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9c64fed6d0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0].view(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.12, accuracy: 0.96\n",
      "loss: 0.09, accuracy: 0.96\n",
      "loss: 0.06, accuracy: 0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.15, grad_fn=<NllLossBackward0>), tensor(0.16))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, ds, shuffle=False):\n",
    "        self.n = n\n",
    "        self.shuffle = shuffle\n",
    "    def __iter__(self):\n",
    "        idxs = list(range(self.n))\n",
    "        if self.shuffle: random.shuffle(idxs)\n",
    "        return iter(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = Sampler(train_ds, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "it = iter(ss)\n",
    "for _ in range(5): print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39614, 16470, 19166, 551, 7498]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Sampler(train_ds, shuffle=True)\n",
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler:\n",
    "    def __init__(self, sampler, bs, drop_last=False): self.sampler, self.bs, self.drop_last = sampler, bs, drop_last\n",
    "    def __iter__(self): yield from fc.chunked(self.sampler, self.bs, drop_last=self.drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[46548, 42175, 12446, 32852],\n",
       " [28931, 22772, 46332, 6296],\n",
       " [15630, 34168, 34914, 40482],\n",
       " [20759, 32337, 14811, 31966],\n",
       " [36817, 1887, 46621, 29818]]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchs = BatchSampler(ss, 4, drop_last=False)\n",
    "list(islice(batchs, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5), tensor(0), tensor(4), tensor(1), tensor(9))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j, k = zip(*[train_ds[i] for i in range(5)])\n",
    "k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's not really anything that needs doing for colate\n",
    "# batch is expected to be a list of len-2-tuples with x,y at each element.\n",
    "# we want to pull out the full batch of x and full batch of y in this collate fn -- collate x, collate y\n",
    "def collate(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    return torch.stack(xs), torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, ds, batchs, collate_fn=collate):\n",
    "        self.ds, self.batchs, self.collate_fn = ds, batchs, collate_fn\n",
    "    def __iter__(self):\n",
    "        yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = BatchSampler(Sampler(train_ds, shuffle=True), bs)\n",
    "valid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batchs=train_samp)\n",
    "valid_dl = DataLoader(valid_ds, batchs=valid_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.19, accuracy: 0.92\n",
      "loss: 0.07, accuracy: 0.98\n",
      "loss: 0.16, accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
