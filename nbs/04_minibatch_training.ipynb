{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Experiments for the basics of nn and torch implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin1')\n",
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "n,m = x_train.shape\n",
    "c = (y_train.max() + 1).item()\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers: x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, nh, c)\n",
    "preds = model(x_train)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Experimenting with the basics for logsoftmax -- yet again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup some vars\n",
    "n_train = 64\n",
    "bs = 4\n",
    "n_classes = 10\n",
    "probs = torch.randn((bs, n_classes))\n",
    "y = torch.randint(0, 9, (bs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.63, -4.00, -2.91, -2.85, -3.18, -1.69, -1.21, -1.24, -5.36, -3.80],\n",
       "        [-2.45, -1.72, -3.00, -2.75, -1.03, -3.16, -4.40, -2.63, -2.25, -3.46],\n",
       "        [-2.01, -4.50, -2.86, -3.37, -2.39, -1.48, -2.49, -1.64, -2.12, -3.04],\n",
       "        [-2.34, -4.65, -2.51, -2.66, -2.41, -3.98, -3.22, -2.17, -1.06, -2.02]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = probs\n",
    "a = x.max(1, keepdim=True).values\n",
    "x_norm = x - a\n",
    "numerator = x_norm\n",
    "logsumexp = x_norm.exp().sum(1, keepdim=True).log()\n",
    "numerator.shape, logsumexp.shape\n",
    "logsoftmax = numerator - logsumexp\n",
    "logsoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.63, -4.00, -2.91, -2.85, -3.18, -1.69, -1.21, -1.24, -5.36, -3.80],\n",
       "        [-2.45, -1.72, -3.00, -2.75, -1.03, -3.16, -4.40, -2.63, -2.25, -3.46],\n",
       "        [-2.01, -4.50, -2.86, -3.37, -2.39, -1.48, -2.49, -1.64, -2.12, -3.04],\n",
       "        [-2.34, -4.65, -2.51, -2.66, -2.41, -3.98, -3.22, -2.17, -1.06, -2.02]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check against a version that doesn't use the normalisation with max\n",
    "(torch.exp(x) / torch.exp(x).sum(1, keepdim=True)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.63, -4.00, -2.91, -2.85, -3.18, -1.69, -1.21, -1.24, -5.36, -3.80],\n",
       "        [-2.45, -1.72, -3.00, -2.75, -1.03, -3.16, -4.40, -2.63, -2.25, -3.46],\n",
       "        [-2.01, -4.50, -2.86, -3.37, -2.39, -1.48, -2.49, -1.64, -2.12, -3.04],\n",
       "        [-2.34, -4.65, -2.51, -2.66, -2.41, -3.98, -3.22, -2.17, -1.06, -2.02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap into function\n",
    "def log_softmax(x):\n",
    "    x_norm = x - x.max(1, keepdim=True).values\n",
    "    numerator = x_norm\n",
    "    logsumexp = x_norm.exp().sum(1, keepdim=True).log()\n",
    "    logsoftmax = numerator - logsumexp\n",
    "    return logsoftmax\n",
    "\n",
    "assert torch.allclose(log_softmax(probs), F.log_softmax(probs, 1)), 'did not find equality with torch in log_softmax implementation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.22) tensor(3.22) tensor(3.22)\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = log_softmax(probs)\n",
    "preds = log_likelihood[range(log_likelihood.shape[0]), y]\n",
    "nll = -preds.mean()\n",
    "\n",
    "print(nll, F.nll_loss(F.log_softmax(probs, 1), y), F.cross_entropy(probs, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap into function\n",
    "def cross_entropy(probs, targets):\n",
    "    log_likelihood = log_softmax(probs)\n",
    "    preds = log_likelihood[range(log_likelihood.shape[0]), targets]\n",
    "    nll = -preds.mean()\n",
    "    return nll\n",
    "\n",
    "assert torch.allclose(cross_entropy(probs, y), F.cross_entropy(probs, y)), 'did not find equality with torch in cross_entropy implementation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Basic training loop\n",
    " Can now just use the torch implementation of log_softmax and cross_entropy\n",
    " - Get model preds\n",
    " - compare against labels and calculate loss\n",
    " - calculate gradient of loss with respece to model params\n",
    " - update params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>),\n",
       " torch.Size([50, 10]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 50\n",
    "xb = x_train[:bs]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[:bs]\n",
    "yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025850929940455"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# About the same as this for random init model and c classes\n",
    "-np.log(1/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3, 9, 3, 5, 3, 8,\n",
       "        3, 5, 9, 5, 9, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
